{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Dependencies to Visualize the model\n",
    "%matplotlib inline\n",
    "from IPython.display import Image, SVG\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Filepaths, numpy, and Tensorflow\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Sklearn scaling\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Keras Specific Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading and Preprocessing our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load the MNIST Handwriting Dataset from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Info\n",
      "Training Data Shape: (60000, 28, 28)\n",
      "Training Data Labels Shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"Training Data Info\")\n",
    "print(\"Training Data Shape:\", X_train.shape)\n",
    "print(\"Training Data Labels Shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Plot the first digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ed35ecd630>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the first image from the dataset\n",
    "plt.imshow(X_train[0,:,:], cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Each Image is a 28x28 Pixel greyscale image with values from 0 to 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our image is an array of pixels ranging from 0 to 255\n",
    "X_train[0, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### For Logistic Regression, we want to flatten our data into rows of 1D image arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (60000, 28, 28, 1)\n",
      "Testing Shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# expected conv2d_1_input to have 4 dimensions\n",
    "\n",
    "\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "print(\"Training Shape:\", X_train.shape)\n",
    "print(\"Testing Shape:\", X_test.shape)\n",
    "\n",
    "# We want to flatten our image of 28x28 pixels to a 1D array of 784 pixels\n",
    "# ndims = X_train.shape[1] * X_train.shape[2]\n",
    "# X_train = X_train.reshape(X_train.shape[0], ndims)\n",
    "# X_test = X_test.reshape(X_test.shape[0], ndims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scaling and Normalization\n",
    "\n",
    "We use Sklearn's MinMaxScaler to normalize our data between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Next, we normalize our training data to be between 0 and 1\n",
    "# scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Alternative way to normalize this dataset since we know that the max pixel value is 255\n",
    "# X_train = X_train.astype(\"float32\")\n",
    "# X_test = X_test.astype(\"float32\")\n",
    "# X_train /= 255.0\n",
    "# X_test /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "We need to one-hot encode our integer labels using the `to_categorical` helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our Training and Testing labels are integer encoded from 0 to 9\n",
    "y_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to convert our target labels (expected values) to categorical data\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "# Original label of `5` is one-hot encoded as `0000010000`\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building our Model\n",
    "\n",
    "In this example, we are going to build a Deep Multi-Layer Perceptron model with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Our first step is to create an empty sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create an empty sequential model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                13530     \n",
      "=================================================================\n",
      "Total params: 13,610\n",
      "Trainable params: 13,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We can summarize our model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compile and Train our Model\n",
    "\n",
    "Now that we have our model architecture defined, we must compile the model using a loss function and optimizer. We can also specify additional training metrics such as accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use categorical crossentropy for categorical data and mean squared error for regression\n",
    "# Hint: your output layer in this example is using software for logistic regression (categorical)\n",
    "# If your output layer activation was `linear` then you may want to use `mse` for loss\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Finally, we train our model using our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Training consists of updating our weights using our optimizer and loss function. In this example, we choose 10 iterations (loops) of training that are called epochs.\n",
    "\n",
    "We also choose to shuffle our training data and increase the detail printed out during each training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/4\n",
      "60000/60000 - 18s - loss: 1.7108 - accuracy: 0.8964\n",
      "Epoch 2/4\n",
      "60000/60000 - 19s - loss: 0.3478 - accuracy: 0.9451\n",
      "Epoch 3/4\n",
      "60000/60000 - 18s - loss: 0.2370 - accuracy: 0.9520\n",
      "Epoch 4/4\n",
      "60000/60000 - 18s - loss: 0.2041 - accuracy: 0.9547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ed362b8630>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit (train) the model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=4,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Saving and Loading models\n",
    "\n",
    "We can save our trained models using the HDF5 binary format with the extension `.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"mnist_trained_cnn.h5\")\n",
    "\n",
    "# Save the model\n",
    "# save model for the web\n",
    "model_json = model.to_json()\n",
    "with open(\"model_cnn.json\", \"w\") as json_file:\n",
    "  json_file.write(model_json)\n",
    "\n",
    "# save trained weights\n",
    "model.save_weights(\"model_cnn_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"mnist_trained_cnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "We use our testing data to validate our model. This is how we determine the validity of our model (i.e. the ability to predict new and previously unseen data points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 2s - loss: 0.2576 - accuracy: 0.9485\n",
      "Loss: 0.25762900595818644, Accuracy: 0.9484999775886536\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the training data \n",
    "model_loss, model_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making Predictions\n",
    "\n",
    "We can use our trained model to make predictions using `model.predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab just one data point to test with\n",
    "test = X_train[1]\n",
    "test = np.expand_dims(test, axis=0)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#plt.imshow(scaler.inverse_transform(test).reshape(28, 28), cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction. The result should be 0000010000000 for a 5\n",
    "# model.predict(test).round()\n",
    "\n",
    "test.shape\n",
    "predictions = model.predict(test)\n",
    "labels = model.predict_classes(test)\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(labels) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import a Custom Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./Images/output.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\pythondata\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\anaconda3\\envs\\pythondata\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ed387deb00>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.misc.pilutil import imread, imresize\n",
    "# read the image into memory\n",
    "x = imread(filepath, mode='L')\n",
    "# make it the right size\n",
    "x = imresize(x, (28, 28))\n",
    "# imsave('final_image.jpg', x)\n",
    "# convert to a 4D tensor to feed into our model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "x = img_to_array(x)\n",
    "x.shape\n",
    "plt.imshow(x.reshape(28, 28), cmap=plt.cm.Greys)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(1, 28, 28, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the image pixels by 255 (or use a scaler from sklearn here)\n",
    "# image /= 255\n",
    "\n",
    "# # Flatten into a 1x28*28 array \n",
    "# img = image.flatten().reshape(-1, 28*28)\n",
    "# img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ed387fb5c0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Invert the pixel values to match the original data\n",
    "x = 1 - x\n",
    "plt.imshow(x.reshape(28, 28), cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = np.expand_dims(image, axis=0)\n",
    "# image.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.6545065e-05 1.0094496e-04 2.5799815e-03 9.7038386e-09 1.8209731e-08\n",
      "  4.1355122e-02 6.1674893e-01 3.3390939e-01 5.2142311e-03 5.4854674e-05]]\n",
      "[6]\n",
      "[6]\n"
     ]
    }
   ],
   "source": [
    "out = model.predict(x)\n",
    "print(out)\n",
    "print(np.argmax(out, axis=1))\n",
    "# convert the response to a string\n",
    "response = np.argmax(out, axis=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
